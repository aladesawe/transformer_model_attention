{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Load normalized sentence pairs\n",
    "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
    "    text_pairs = pickle.load(fp)\n",
    "\n",
    "# train-test-val split of randomized sentences pairs\n",
    "random.shuffle(text_pairs)\n",
    "n_val = int(0.15*len(text_pairs))\n",
    "n_train = len(text_pairs) - 2*n_val\n",
    "train_pairs = text_pairs[:n_train]\n",
    "val_pairs = text_pairs[n_train:n_train+n_val]\n",
    "test_pairs = text_pairs[n_train+n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 14:42:07.844509: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Parameter determined after analyzing the input data\n",
    "vocab_size_en = 10000\n",
    "vocab_size_fr = 20000\n",
    "seq_length = 20\n",
    "\n",
    "# Create vectorizer\n",
    "eng_vectorizer = TextVectorization(\n",
    "    max_tokens=vocab_size_en,\n",
    "    standardize=None,\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=seq_length,\n",
    ")\n",
    "fra_vectorizer = TextVectorization(\n",
    "    max_tokens=vocab_size_fr,\n",
    "    standardize=None,\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=seq_length + 1,\n",
    ")\n",
    "\n",
    "# train the vectorization layer using training dataset\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_fra_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorizer.adapt(train_eng_texts)\n",
    "fra_vectorizer.adapt(train_fra_texts)\n",
    "\n",
    "# save for subsequent steps\n",
    "with open(\"vectorize.pickle\", \"wb\") as fp:\n",
    "    data = {\n",
    "        \"train\": train_pairs,\n",
    "        \"val\": val_pairs,\n",
    "        \"test\": test_pairs,\n",
    "        \"engvec_config\": eng_vectorizer.get_config(),\n",
    "        \"engvec_weights\": eng_vectorizer.get_weights(),\n",
    "        \"fravec_config\": fra_vectorizer.get_config(),\n",
    "        \"fravec_weights\": fra_vectorizer.get_weights(),\n",
    "    }\n",
    "    pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
      "inputs[\"encoder_inputs\"][0]: [264  30 585  80   6  50  26 185 698 117   2   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
      "inputs[\"decoder_inputs\"][0]: [   2   14  809 2928   48  219   78   59  163 1625   10   13  113    4\n",
      "    3    0    0    0    0    0]\n",
      "targets.shape: (64, 20)\n",
      "targets[0]: [  14  809 2928   48  219   78   59  163 1625   10   13  113    4    3\n",
      "    0    0    0    0    0    0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 16:11:33.367772: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_15' with dtype resource\n",
      "\t [[{{node Placeholder/_15}}]]\n",
      "2024-03-15 16:11:33.368103: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_12' with dtype int64\n",
      "\t [[{{node Placeholder/_12}}]]\n",
      "2024-03-15 16:11:33.410685: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-03-15 16:11:33.437519: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# load text data and vectorizer weights\n",
    "with open(\"vectorize.pickle\", \"rb\") as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "train_pairs = data[\"train\"]\n",
    "val_pairs = data[\"val\"]\n",
    "\n",
    "eng_vectorizer = TextVectorization.from_config(data[\"engvec_config\"])\n",
    "eng_vectorizer.set_weights(data[\"engvec_weights\"])\n",
    "fra_vectorizer = TextVectorization.from_config(data[\"fravec_config\"])\n",
    "fra_vectorizer.set_weights(data[\"fravec_weights\"])\n",
    "\n",
    "# set up Dataset object\n",
    "def format_dataset(eng, fra):\n",
    "    \"\"\"Take an English and a French sentence pair, convert into input and target.\n",
    "    The input is a dict with keys `encoder_inputs` and `decoder_inputs`, each\n",
    "    is a vector, corresponding to English and French sentences respectively.\n",
    "    The target is also vector of the French sentence, advanced by 1 token. All\n",
    "    vector are in the same length.\n",
    "    The output will be used for training the transformer model. In the model we\n",
    "    will create, the input tensors are named `encoder_inputs` and `decoder_inputs`\n",
    "    which should be matched to the keys in the dictionary for the source part\n",
    "    \"\"\"\n",
    "    eng = eng_vectorizer(eng)\n",
    "    fra = fra_vectorizer(fra)\n",
    "    source = {\"encoder_inputs\": eng,\n",
    "                \"decoder_inputs\": fra[:, :-1]}\n",
    "    target = fra[:, 1:]\n",
    "    return (source, target)\n",
    "\n",
    "def make_dataset(pairs, batch_size=64):\n",
    "    \"\"\"Create TensorFlow Dataset for the sentence pairs\"\"\"\n",
    "    # aggregate sentences using zip(*pairs)\n",
    "    eng_texts, fra_texts = zip(*pairs)\n",
    "    # convert them into list, and then create tensors\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(fra_texts)))\n",
    "    return dataset.shuffle(2048) \\\n",
    "                    .batch(batch_size).map(format_dataset) \\\n",
    "                    .prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "# test the dataset\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"encoder_inputs\"][0]: {inputs[\"encoder_inputs\"][0]}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"][0]: {inputs[\"decoder_inputs\"][0]}')\n",
    "    print(f\"targets.shape: {targets.shape}\")\n",
    "    print(f\"targets[0]: {targets[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
